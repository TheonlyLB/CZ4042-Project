{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61884c60-07d6-41da-9e9e-7e53b49b9252",
   "metadata": {},
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "403e248b-e7c7-4dae-ad06-7161fb402dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import timm\n",
    "import torchvision.transforms as tfms\n",
    "import numpy as np\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83f1e1c-c534-40fc-8057-01bd8b0da238",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "156e08dd-21f0-442b-9845-c994c1230b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"datasets\"\n",
    "# Run below cell if dataset not downloaded yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c5680e-baa3-466a-99ce-1ac79e75e8a5",
   "metadata": {},
   "source": [
    "### Run below cell if dataset not downloaded yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bffbe2-1bf9-4553-90dd-2b65715067a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_root = \"datasets\"\n",
    "\n",
    "# base_url = \"https://graal.ift.ulaval.ca/public/celeba/\"\n",
    "\n",
    "# file_list = [\n",
    "#     \"img_align_celeba.zip\",\n",
    "#     \"list_attr_celeba.txt\",\n",
    "#     \"identity_CelebA.txt\",\n",
    "#     \"list_bbox_celeba.txt\",\n",
    "#     \"list_landmarks_align_celeba.txt\",\n",
    "#     \"list_eval_partition.txt\",\n",
    "# ]\n",
    "\n",
    "# # Path to folder with the dataset\n",
    "# dataset_folder = f\"{data_root}/celeba\"\n",
    "# os.makedirs(dataset_folder, exist_ok=True)\n",
    "\n",
    "# for file in file_list:\n",
    "#     url = f\"{base_url}/{file}\"\n",
    "#     if not os.path.exists(f\"{dataset_folder}/{file}\"):\n",
    "#         wget.download(url, f\"{dataset_folder}/{file}\")\n",
    "\n",
    "# with zipfile.ZipFile(f\"{dataset_folder}/img_align_celeba.zip\", \"r\") as ziphandler:\n",
    "#     ziphandler.extractall(dataset_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a98df4a-62fb-46c5-85f6-ff1b13c350e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The running processor is... cuda\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "image_size = 224\n",
    "w, h = 218, 178  # the width and the hight of original images before resizing\n",
    "gender_index = 20  # in the CelebA dataset gender information is the 21th item in the attributes vector.\n",
    "imagenet_mean = [0.485, 0.456, 0.406]  # mean of the ImageNet dataset for normalizing\n",
    "imagenet_std = [0.229, 0.224, 0.225]  # std of the ImageNet dataset for normalizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7c9d092-a2a3-43e5-9763-7896107d70e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = tfms.Compose(\n",
    "    [\n",
    "        tfms.Resize((image_size, image_size)),\n",
    "        tfms.ToTensor(),\n",
    "        tfms.Normalize(imagenet_mean, imagenet_std),\n",
    "    ]\n",
    ")\n",
    "train_dataset = datasets.CelebA(data_root, split=\"train\", target_type=[\"attr\"], transform=transforms)\n",
    "valid_dataset = datasets.CelebA(data_root, split=\"valid\", target_type=[\"attr\"], transform=transforms)\n",
    "test_dataset = datasets.CelebA(data_root, split=\"test\", target_type=[\"attr\"], transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb65d58f-160d-4fe2-9c28-57d89cf46edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only taking part of the dataset to train\n",
    "train_num = 5000\n",
    "train_ratio = 0.7\n",
    "\n",
    "train_subset = Subset(train_dataset, np.arange(1, int(train_num*train_ratio)))\n",
    "valid_subset = Subset(valid_dataset, np.arange(1, int(train_num*(1-train_ratio))))\n",
    "test_subset = Subset(test_dataset, np.arange(1, 1000))\n",
    "train_dataloader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586dae17-aef2-4a16-888f-deb0caea2fb4",
   "metadata": {},
   "source": [
    "# Custom Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac2d7cda-f3a2-439e-8466-d6cccebeeca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationRegressionLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ClassificationRegressionLoss, self).__init__()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()  # size_average=False\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        loss_cls = self.ce_loss(y_pred, y_true[:, gender_index])  # Cross Entropy Error (for classification)\n",
    "        return loss_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbca5f21-30b4-4d3b-a520-9b24dbbcd47f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe78bc-06cf-4722-ad4b-fded0ad8760c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"The running processor is...\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c7fedc0-3125-4126-9b7d-1cf489d401fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "criterion = ClassificationRegressionLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9bf7e8f-d19a-4341-8911-cc083b629594",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = ['inception_v4', 'inception_resnet_v2']\n",
    "learning_rate_list = [0.01,0.005,0.001]\n",
    "optimizer_list = ['SGD','RMSprop', 'Adam'] # 'SGD'\n",
    "momentum_list = [0.0, 0.2, 0.4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "648dcbf8-9e2e-4354-8336-83464aef96a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "for model in model_list:\n",
    "    # Initialize the Inception model\n",
    "    model = timm.create_model(model, pretrained=True, num_classes=2)\n",
    "    model.to(device)\n",
    "    break\n",
    "    \n",
    "    for optimizer_name in optimizer_list:\n",
    "        for learning_rate in learning_rate_list:\n",
    "          for momentum in momentum_list:\n",
    "        \n",
    "            if optimizer_name == 'SGD':\n",
    "              optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "            elif optimizer_name == 'RMSprop':\n",
    "              optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "            elif optimizer_name == 'Adam':\n",
    "              optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "            log_filename = f'saves/inceptionV4_{optimizer_name}_{momentum}_{learning_rate}.tsv'\n",
    "            \n",
    "            with open(log_filename, 'w', newline='') as log_file:\n",
    "                log_writer = csv.writer(log_file, delimiter='\\t')\n",
    "            \n",
    "                # Write the header row to the log file\n",
    "                log_writer.writerow(['epoch', 'time', 'lr', 'loss', 'val_loss'])\n",
    "            \n",
    "                for epoch in range(num_epochs):\n",
    "                    epoch_start_time = time.time()\n",
    "                    for phase in ['train', 'val']:\n",
    "                        start_time = time.time()\n",
    "                        if phase == 'train':\n",
    "                            model.train()\n",
    "                            dataloader = train_dataloader\n",
    "                        else:\n",
    "                            model.eval()\n",
    "                            dataloader = valid_dataloader\n",
    "                \n",
    "                        running_loss = 0.0\n",
    "                        corrects = 0\n",
    "                \n",
    "                        for inputs, labels in dataloader:\n",
    "                            inputs = inputs.to(device)\n",
    "                            labels = labels.to(device)\n",
    "                \n",
    "                            optimizer.zero_grad()\n",
    "                \n",
    "                            with torch.set_grad_enabled(phase == 'train'):\n",
    "                                outputs = model(inputs)\n",
    "                                loss = criterion(outputs, labels)\n",
    "                \n",
    "                                if phase == 'train':\n",
    "                                    loss.backward()\n",
    "                                    optimizer.step()\n",
    "                \n",
    "                            # Calculate the number of correct predictions for binary classification\n",
    "                            _, preds = torch.max(outputs, 1)\n",
    "                            \n",
    "                            # Assuming labels.data is a 2D tensor of shape [batch_size, num_classes], you need to extract the binary class labels\n",
    "                            # If labels.data contains multiple columns, you can get the binary class labels from one of the columns\n",
    "                            binary_labels = labels.data[:, gender_index].to(device)\n",
    "                        \n",
    "                            # Calculate the number of correct predictions\n",
    "                            corrects += torch.sum(preds == binary_labels)\n",
    "                        \n",
    "                            running_loss += loss.item() * inputs.size(0)\n",
    "                \n",
    "                        end_time = time.time()\n",
    "                        cur_epoch_time = end_time - start_time\n",
    "            \n",
    "                        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "                        \n",
    "                        if phase == 'train':\n",
    "                            train_loss = epoch_loss\n",
    "                        else:\n",
    "                            val_loss = epoch_loss\n",
    "                            \n",
    "                        epoch_acc = corrects.double() / len(dataloader.dataset)\n",
    "                        print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "                        print(f'Time taken for {phase} epoch: {cur_epoch_time:.2f} seconds')\n",
    "                    \n",
    "                    full_epoch_time = time.time() - epoch_start_time\n",
    "                    # Record the learning rate\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    log_writer.writerow([epoch + 1, full_epoch_time, current_lr, train_loss, val_loss])  # You need to compute validation loss\n",
    "\n",
    "                # Save the trained model\n",
    "                # torch.save(model.state_dict(), 'inceptionv4_celeba.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (pytorch_env)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
